{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d8a42c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm # track progress\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "\n",
    "def occlude_face_images(input_path_img, input_path_annotations, output_path, batch_size=0, mem_clean_after=100):\n",
    "    img_list = os.listdir(input_path_img) # list of all images in folder\n",
    "    img_list = natsorted(img_list)\n",
    "\n",
    "    batch_counter = 0\n",
    "    # Create a tqdm progress bar\n",
    "    total_images = min(len(img_list), batch_size if batch_size != 0 else batch_size + 1)\n",
    "    progress_bar = tqdm(total=total_images, desc='Processing Images', unit='images')\n",
    "    \n",
    "    # Create checkpoint file\n",
    "    chkpt = os.path.join(output_path, \"checkpoint.txt\")\n",
    "    if os.path.exists(chkpt):\n",
    "        with open(chkpt, 'r') as f:\n",
    "            last_processed = f.readline().strip()\n",
    "    else:\n",
    "        last_processed = ''\n",
    "    \n",
    "    # Get index of last processed file\n",
    "    last_processed_index = 0 if last_processed == '' else img_list.index(last_processed)\n",
    "    \n",
    "    for i in range(last_processed_index + 1, len(img_list)):\n",
    "        if batch_counter == batch_size and batch_counter > 0:\n",
    "            break\n",
    "        \n",
    "        filename = os.path.splitext(img_list[i])[0] # splits filename from extension ['filename', '.ext']\n",
    "        ext = os.path.splitext(img_list[i])[1]\n",
    "\n",
    "        # Load facial landmarks from the npy file\n",
    "        l = np.load(os.path.join(input_path_annotations, filename + '_lnd.npy'))\n",
    "\n",
    "        # Convert landmarks to a 2D array with shape (68, 2)\n",
    "        l = l.reshape((68, 2))\n",
    "\n",
    "        # Load corresponding face image\n",
    "        img = mpimg.imread(os.path.join(input_path_img, img_list[i]))\n",
    "\n",
    "        # Get original image dimensions\n",
    "        original_height, original_width, _ = img.shape\n",
    "\n",
    "        # Create a figure with the same dimensions as the original image\n",
    "        fig = plt.figure(figsize=(original_width / 100, original_height / 100), dpi=100)\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Calculate the distance between facial landmark 0 and facial landmark 16\n",
    "        # norm = vector length, euclidean distance = distance\n",
    "        w = np.linalg.norm(l[0] - l[16])\n",
    "\n",
    "        # meta quest 2 aspect ratio (width/height)\n",
    "        ar = 224/105\n",
    "\n",
    "        # Calculate the height of the rectangle\n",
    "        h = w / (ar)\n",
    "\n",
    "        # Calculate the center point between facial landmark 36 and facial landmark 45 (eyes)\n",
    "        m_ocd = l[36] + (l[45] - l[36]) / 2\n",
    "\n",
    "        # Calculate the angle between facial landmark 36 and facial landmark 45\n",
    "        a = l[45, 0] - l[36, 0] # Ankathete, x-axis\n",
    "        b = l[45, 1] - l[36, 1] # Gegenkathete, y-axis\n",
    "        alpha = np.arctan2(b, a)\n",
    "\n",
    "        # Calculate the corner point of the rectangle\n",
    "        R_a = m_ocd - w / 2 * np.array([np.cos(alpha), np.sin(alpha)]) - h / 2 * np.array([-np.sin(alpha), np.cos(alpha)])\n",
    "\n",
    "        # Create and add the rectangle patch to the plot\n",
    "        rect = patches.Rectangle((R_a[0], R_a[1]), w, h, angle=np.degrees(alpha), facecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Save File\n",
    "        plt.savefig(output_path + filename + '_occ.jpg', format='jpg', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "        # Remove Image from Memory\n",
    "        plt.close(fig)\n",
    "        \n",
    "        #if batch_counter % mem_clean_after == 0: \n",
    "        #    del(img)\n",
    "        #    del(fig)\n",
    "        #    gc.collect()\n",
    "        \n",
    "        # Update the checkpoint after processing each file\n",
    "        with open(chkpt, 'w') as f:\n",
    "            f.write(filename + ext)\n",
    "        batch_counter += 1\n",
    "        progress_bar.update(1) # Update progress bar\n",
    "    \n",
    "    # Close progress bar when done\n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4b2d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset paths\n",
    "\n",
    "#affnet_img = '../AffectNet/train_set/images/'\n",
    "#affnet_anno = '../AffectNet/train_set/annotations/'\n",
    "#output_path = '../AffectNet_Occluded/train_set/images/'\n",
    "\n",
    "#affnet_img = r'C:\\Users\\LEAND\\Coding\\_FER\\AffectNet\\train_set\\images\\\\'\n",
    "#affnet_anno = r'C:\\Users\\LEAND\\Coding\\_FER\\AffectNet\\train_set\\annotations\\\\'\n",
    "#output_path = r'C:\\Users\\LEAND\\Coding\\_FER\\AffectNet_Occluded\\train_set\\images\\\\'\n",
    "\n",
    "affnet_img = r'C:\\Users\\LEAND\\Coding\\_FER\\AffectNet\\val_set\\images\\\\'\n",
    "affnet_anno = r'C:\\Users\\LEAND\\Coding\\_FER\\AffectNet\\val_set\\annotations\\\\'\n",
    "output_path = r'C:\\Users\\LEAND\\Coding\\_FER\\AffectNet_Occluded\\val_set\\images\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7607de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|█████████████████████████████████████████████████████| 10000/10000 [06:59<00:00, 23.81images/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%timeit\n",
    "occlude_face_images(affnet_img, affnet_anno, output_path, batch_size=10000)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbd7419a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21700000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46b93179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|█████████████████████████████████████████████████████| 10000/10000 [07:00<00:00, 23.76images/s]\n"
     ]
    }
   ],
   "source": [
    "occlude_face_images(affnet_img, affnet_anno, output_path, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5a78811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21702409"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4c66355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|█████████████████████████████████████████████████████| 10000/10000 [07:00<00:00, 23.75images/s]\n"
     ]
    }
   ],
   "source": [
    "occlude_face_images(affnet_img, affnet_anno, output_path, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82ef7f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21701895"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "082fac17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|█████████████████████████████████████████████████████| 10000/10000 [07:01<00:00, 23.73images/s]\n"
     ]
    }
   ],
   "source": [
    "occlude_face_images(affnet_img, affnet_anno, output_path, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3680468d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21701675"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "058287bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|█████████████████████████████████████████████████████| 10000/10000 [07:09<00:00, 23.30images/s]\n"
     ]
    }
   ],
   "source": [
    "occlude_face_images(affnet_img, affnet_anno, output_path, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3774b75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21702109"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7f459b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|█████████████████████████████████████████████████████| 10000/10000 [07:07<00:00, 23.37images/s]\n"
     ]
    }
   ],
   "source": [
    "occlude_face_images(affnet_img, affnet_anno, output_path, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be0aa63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21702245"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "219880a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|█████████████████████████████████████████████████████| 10000/10000 [07:09<00:00, 23.27images/s]\n"
     ]
    }
   ],
   "source": [
    "occlude_face_images(affnet_img, affnet_anno, output_path, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d45bfb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21702146"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05419feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|█████████████████████████████████████████████████████| 10000/10000 [07:11<00:00, 23.17images/s]\n"
     ]
    }
   ],
   "source": [
    "occlude_face_images(affnet_img, affnet_anno, output_path, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6484221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21701884"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffa15e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|█████████████████████████████████████████████████████| 10000/10000 [07:19<00:00, 22.77images/s]\n"
     ]
    }
   ],
   "source": [
    "occlude_face_images(affnet_img, affnet_anno, output_path, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4634a1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21702440"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fc3c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████████████████████████████████████████████████▉| 3998/3999 [02:35<00:00, 25.75images/s]\n"
     ]
    }
   ],
   "source": [
    "occlude_face_images(affnet_img, affnet_anno, output_path, batch_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a1773f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8676475"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ddf312",
   "metadata": {},
   "outputs": [],
   "source": [
    "287651"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
