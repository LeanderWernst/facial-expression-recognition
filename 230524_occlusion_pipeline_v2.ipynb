{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a42c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "#from tqdm import tqdm # track progress\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "\n",
    "def occlude_face_images(input_path_img, input_path_annotations, output_path, batch_size=0):\n",
    "    img_list = os.listdir(input_path_img) # list of all images in folder\n",
    "    img_list = natsorted(img_list)\n",
    "\n",
    "    batch_counter = 0\n",
    "    # Create a tqdm progress bar\n",
    "    total_images = min(len(img_list), batch_size)\n",
    "    #progress_bar = tqdm(total=total_images, desc='Processing Images', unit='images')\n",
    "    \n",
    "    # Create checkpoint file\n",
    "    chkpt = os.path.join(output_path, \"checkpoint.txt\")\n",
    "    if os.path.exists(chkpt):\n",
    "        with open(chkpt, 'r') as f:\n",
    "            last_processed = f.readline().strip()\n",
    "    else:\n",
    "        last_processed = ''\n",
    "    \n",
    "    # Get index of last processed file\n",
    "    last_processed_index = 0 if last_processed == '' else img_list.index(last_processed)\n",
    "    \n",
    "    for i in range(last_processed_index, len(img_list)):\n",
    "        if batch_counter == batch_size and batch_counter > 0:\n",
    "            break\n",
    "        \n",
    "        filename = os.path.splitext(img_list[i])[0] # splits filename from extension ['filename', '.ext']\n",
    "        ext = os.path.splitext(img_list[i])[1]\n",
    "\n",
    "        # Load facial landmarks from the npy file\n",
    "        l = np.load(os.path.join(input_path_annotations, filename + '_lnd.npy'))\n",
    "\n",
    "        # Convert landmarks to a 2D array with shape (68, 2)\n",
    "        l = l.reshape((68, 2))\n",
    "\n",
    "        # Load corresponding face image\n",
    "        img = mpimg.imread(os.path.join(input_path_img, img_list[i]))\n",
    "\n",
    "        # Get original image dimensions\n",
    "        original_height, original_width, _ = img.shape\n",
    "\n",
    "        # Create a figure with the same dimensions as the original image\n",
    "        fig = plt.figure(figsize=(original_width / 100, original_height / 100), dpi=100)\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Calculate the distance between facial landmark 0 and facial landmark 16\n",
    "        # norm = vector length, euclidean distance = distance\n",
    "        w = np.linalg.norm(l[0] - l[16])\n",
    "\n",
    "        # meta quest 2 aspect ratio (width/height)\n",
    "        ar = 224/105\n",
    "\n",
    "        # Calculate the height of the rectangle\n",
    "        h = w / (ar)\n",
    "\n",
    "        # Calculate the center point between facial landmark 36 and facial landmark 45 (eyes)\n",
    "        m_ocd = l[36] + (l[45] - l[36]) / 2\n",
    "\n",
    "        # Calculate the angle between facial landmark 36 and facial landmark 45\n",
    "        a = l[45, 0] - l[36, 0] # Ankathete, x-axis\n",
    "        b = l[45, 1] - l[36, 1] # Gegenkathete, y-axis\n",
    "        alpha = np.arctan2(b, a)\n",
    "\n",
    "        # Calculate the corner point of the rectangle\n",
    "        R_a = m_ocd - w / 2 * np.array([np.cos(alpha), np.sin(alpha)]) - h / 2 * np.array([-np.sin(alpha), np.cos(alpha)])\n",
    "\n",
    "        # Create and add the rectangle patch to the plot\n",
    "        rect = patches.Rectangle((R_a[0], R_a[1]), w, h, angle=np.degrees(alpha), facecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Save File\n",
    "        plt.savefig(output_path + filename + '_occ.jpg', format='jpg', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "        # Remove Image from Memory\n",
    "        plt.close(fig)\n",
    "        del(img)\n",
    "        gc.collect()\n",
    "        \n",
    "        # Update the checkpoint after processing each file\n",
    "        with open(chkpt, 'w') as f:\n",
    "            f.write(filename + ext)\n",
    "        batch_counter += 1\n",
    "        #progress_bar.update(1) # Update progress bar\n",
    "    \n",
    "    # Close progress bar when done\n",
    "    #progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset paths\n",
    "#affnet_img = '../AffectNet/train_set/images/'\n",
    "#affnet_anno = '../AffectNet/train_set/annotations/'\n",
    "#output_path = '../AffectNet_Occluded/train_set/images/'\n",
    "\n",
    "affnet_img = r'C:\\Users\\LEAND\\Coding\\_FER\\AffectNet\\train_set\\images\\\\'\n",
    "affnet_anno = r'C:\\Users\\LEAND\\Coding\\_FER\\AffectNet\\train_set\\annotations\\\\'\n",
    "output_path = r'C:\\Users\\LEAND\\Coding\\_FER\\AffectNet_Occluded\\train_set\\images\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7607de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "occlude_face_images(affnet_img, affnet_anno, output_path, batch_size=45000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7419a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
